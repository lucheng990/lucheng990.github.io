---
tag: 机器学习
---





# Support Vector Machine

SVM有两个特色:

1. 使用了Hinge Loss
2. 使用了Kernel Method



## Hinge loss

在之前的Binary Classification中，理想的做法是：

- Function Set

$$
g(x) = \begin{cases}
f(x)>0 \quad & \mathrm{Output} = +1 \\
f(x)<0 \quad & \mathrm{Output} = -1
\end{cases}
$$

对于每一笔training data \\(x^n\\)，都有相对应的\\(\hat{y}^n\\)，其中
$$
\hat{y}^n = +1,-1
$$
在logistic regression里面的\\(\hat{y}^n\\)是用0和1来表示的，但是这里用+1和-1 。两者本质上没有区别，这里这样设置对后面的式子化简有帮助。

- Loss function

理想的loss function是下面这样：
$$
L(f) = \sum_n \delta (g(x^n)\ne \hat{y}^n)
$$


Loss is the number of times g get incorrect results on training data.





- Training by gradient descent



在这个task中，Optimise这一步会很难，因为上面的Loss function不能够求微分，所以无法用gradient descent来求解。



**为了解决无法微分这一问题，可以用一个approximate的function来代替这个\\(\delta\\)。同时，新function的表达式中不直接含有\\(g(x)\\)。**


$$
L(f) = \sum_n l(f(x^n),\hat{y}^n)
$$


现在的任务是要optimize这个新的loss function。



## Back to Step 2

**新的Loss function长什么样需要自己来定。**



![a1p4](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/19SVM%281%29/a1p4.png)

如上图，先不看几个各种颜色的函数，只看坐标轴。



图上的**横轴**是\\(\hat{y}^n\\)乘上\\(f(x)\\)，也就是\\(\hat{y}^n f(x)\\)。

图上的**纵轴**就是loss value。

黑色线为ideal loss function，但是不能求微分。

新的loss function要能够体现出原来的\\(g(x)\\)。重复一遍
$$
g(x) = \begin{cases}
f(x)>0 \quad & \mathrm{Output} = +1 \\
f(x)<0 \quad & \mathrm{Output} = -1
\end{cases}
$$
在这个式子中，最好最精确的情况显然是：

- 在\\(\hat{y}^n\\)为1的时候，希望\\(f(x^n)\\)越大越好。
- 在\\(\hat{y}^n\\)为-1的时候，希望\\(f(x^n)\\)越负越好。

可以对应到坐标轴上，如果\\(\hat{y}^n f(x)\\)的值越大（往横轴正方向走）的话，loss value应该越小。

**下面分别采取几个不同的function: \\(l(f(x^n),\hat{y}^n)\\)**

### Square Loss

Square Loss的做法是，

- If \\(\hat{y}^n = 1, \quad f(x)\\) close to 1
- if \\(\hat{y}^n = -1, \quad f(x)\\) close to -1



也就是说，

- 当\\(\hat{y}^n\\)为1的时候，\\(f(x^n)-1\\)要越接近0越好。
- 当\\(\hat{y}^n\\)为-1的时候，\\(f(x^n)-(-1)\\)要越接近0越好。

如果取个平方，那么两个式子刚好可以写成同一个：
$$
l(f(x^n),\hat{y}^n) = (\hat{y}^n f(x^n) -1)^2
$$
如上图红色曲线所示。

**但是这种做法显然是不合理的，因为从左往右过了对称轴loss value反而会增大**



### Sigmoid + Square Loss

Sigmoid + Square Loss的做法是：

- If \\(\hat{y}^n\\) = 1,     \\(\sigma (f(x^n))\\) close to 1
- If \\(\hat{y}^n\\) = -1,   \\(\sigma (f(x^n)) \\) close to 0



Loss Function可以写成：
$$
l(f(x^n),\hat{y}^n) = (\sigma(\hat{y} f(x^n))-1)^2
$$
因为，如果\\(\hat{y}^n\\)为1的话，上面的式子就变成了：
$$
(\sigma(f(x^n)) -1)^2
$$
能够表达越接近1越好这个意思；



如果\\(\hat{y}^n\\)为-1的话，上面的式子就变成了：
$$
(\sigma(-f(x))-1)^2
$$
根据sigmoid函数的特性，\\(\sigma(-f(x)) = 1-\sigma(f(x))\\)，所以上式就变成了：
$$
(1-\sigma(f(x))-1)^2=(\sigma(f(x))-0)^2
$$
能够表达越接近0越好的这个意思。



**如上图蓝色线所示。**



### Sigmoid + Cross Entropy

在做logistic regression的时候，一般不会用square loss作为loss function，而会选择使用cross entropy，这样效果会好很多。所以在这里使用了Sigmoid + Cross Entropy



可以得到Loss Function如下：
$$
l(f(x^n),\hat{y}^n) = \ln(1+\exp(-\hat{y}^n f(x)))
$$
**这个Loss Function可以除以一个ln2（对loss function除一个constant不会影响最终结果），这样可以让它变成ideal loss（黑色线）的upper bound。**

**除以ln2以后的曲线如上图绿色线所示。**



### 不采取Square Error而选择Cross Entropy的原因



![a2p7](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/19SVM%281%29/a2p7.png)



现在将\\(\hat{y}^n f(x)\\)从-2移动到-1，如果选择Square error，那么Loss value变化的幅度将会很小，而选择cross entropy变化就会很大。



现在假设\\(\hat{y}^n f(x)\\)的取值非常极端(非常negative)，按照道理应该要具有非常大的gradient，但是使用square error就不是如此。相反cross entropy的效果会很好。





### Hinge Loss

Hinge Loss的formulation如下：
$$
l(f(x^n),\hat{y}^n) = \max(0,1-\hat{y}^n f(x))
$$




- 如果\\(\hat{y}^n\\) = 1

Hinge Loss就变成了,
$$
\max(0,1-f(x))
$$
要让loss变为0，那么就需要\\(1-f(x)<0\\)，也就是\\(f(x)>1\\)。

- 如果\\(\hat{y}^n\\) = -1

Hinge Loss就变成了,
$$
\max(0,1+f(x))
$$
要让loss变为0，需要\\(1+f(x)<0\\)，也就是\\(f(x)<-1\\)。



![a3p8](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/19SVM%281%29/a3p8.png)

如上图，hinge loss就是紫色的那条线。只要横坐标大于1，Loss就变成了0,此时横坐标再大也没有帮助了。

对于这个classification的任务来说，只要横坐标大于0就能够分类出来。在hinge loss中，这还不够好，需要再好过一段距离。只要横坐标没有大于1，都还会有penalty，促使\\(\hat{y}^n f(x)\\)大于1 。

hinge loss和cross entropy之间的最大不同，在于横坐标大于1(margin)的部分（这一部分已经能够作出classification了）。

在实做上，两者的差别不是很大，有些时候可以看到hinge loss比cross entropy会更好（略胜）。**hinge loss受到outlier的影响会更小。**

这里选择'1'，是因为这样能够让这条线变成ideal loss的一个tight upper bound。

**现在只要minimize hinge loss，就能够得到minimize ideal loss的效果**







## Linear SVM 

Linear SVM，指的是function（model）就是linear的，在step1中

### Step 1: Function(Model)

$$
f(x) = \sum_i w_ix_i +b 
$$

可以把这件事情看成是两个vector的inner product。
$$
\begin{bmatrix}
w \\
b
\end{bmatrix}
\cdot
\begin{bmatrix}
x \\
1
\end{bmatrix}
$$
将这两个concatenate起来的vector看成新的vector。上面的式子又可以看成是（矩阵乘法）：
$$
w^Tx
$$
在SVM里面，当\\(f(x)\\)大于0的时候属于某个class，当\\(f(x)\\)小于0的时候属于另外一个class。

### Step 2: Loss Function

SVM的Loss Function的特色是采用了Hinge loss，通常还会加上一个regularization term：


$$
L(f) = \sum_n l(f(x^n),\hat{y}^n) + \lambda \lVert w \rVert_2
$$


其中，


$$
l(f(x^n),\hat{y}^n) = \max(0,1-\hat{y}^n f(x))
$$
Hinge loss的function和regularization的function都是convex的，所以这一总的Loss Function也是一个凸函数。

convex loss function使用gradient descent的方法很容易训练出来。

### Step 3: gradient descent



convex function以及几个convex function的叠加都会产生很多不可微的分界点，但是并不影响使用gradient descent去训练。



**Step 1中的function不一定要是linear的，svm也可以有deep的版本**



> Yichuan Tang,"Deep Learning using Linear Support Vector Machines", ICML 2013 Challenges in Representation Learning Workshop





### 使用gradient descent来train SVM

SVM的训练方法有很多种，gradient descent是其中一种。

先忽略掉regularization term的部分。loss function如下：


$$
L(f) = \sum_n l(f(x^n),\hat{y}^n)
$$
其中，
$$
l(f(x^n),\hat{y}^n) = \max(0,1-\hat{y} f(x^n))
$$


对某一个weight(\\(w_i\\))做偏微分：


$$
\frac{\partial l(f(x^n),\hat{y}^n)}{\partial w_i} = \frac{\partial l(f(x^n),\hat{y}^n)}{\partial f(x^n)} \frac{\partial f(x^n)}{\partial w_i} 
$$
其中，\\(f(x^n) = w^T \cdot x^n\\)，所以：


$$
\frac{\partial f(x^n)}{\partial w_i} = x^n_i
$$


而上面式子的前半部分是：




$$
\frac{\partial \max(0,1-\hat{y}^n f(x^n))}{\partial f(x^n)} = \begin{cases}
-\hat{y}^n \quad \mathrm{if} \, \hat{y}^n f(x^n)<1 \\
0 \qquad \, \mathrm{otherwise}
\end{cases}
$$


总的来看，loss function对某一个weight的偏微分的值等于：


$$
\frac{\partial L(f)}{\partial w_i} = \sum_n -\delta(\hat{y}^n f(x^n) <1) \hat{y}^n x_i^n
$$


其中，\\(-\delta(\hat{y}^n f(x^n) <1) \hat{y}^n\\) depend on现在的参数，所以可以写成：\\(c^n(w)\\)。参数更新可以如下：


$$
w_i \gets w_i - \eta \sum_n c^n(w) x^n
$$


### Linear SVM - another formulation

平常看到的SVM和刚才的式子有点区别（hinge loss上有点区别）。



要Minimizing的loss function L:
$$
L(f) = \sum_n l(f(x^n),\hat{y}^n) +\lambda \lVert w \rVert_2
$$
将hinge loss记为notation \\(\varepsilon\\)，
$$
l(f(x^n),\hat{y}^n) = \varepsilon^n = \max(0,1-\hat{y}^n f(x^n))
$$


现在的目标是要minimize total loss，将刚才的\\(\varepsilon^n\\)这一项写成：


$$
\begin{cases}
\varepsilon^n  \ge 0 \\
\varepsilon^n \ge 1-\hat{y}^n f(x^n) \Rightarrow \hat{y}^nf(x^n) \ge 1-\varepsilon^n
\end{cases}
$$
而，
$$
L(f) = \sum_n \varepsilon^n + \lambda \lVert w \rVert_2
$$


在minimizing loss function的时候加上这一个constraint，即：既要最小化\\(\varepsilon^n\\)，又要让\\(\varepsilon^n\\)满足这两个constraint。这是一个Quadradic programming problem（二次规划问题）。称这个\\(\varepsilon^n\\)为slack variable。



在这种情况下，原来的式子和现在的包含两个大于等于的式子就变得一样了。





## Dual Representation

实际上，minimize loss所找出来的那一组weight \\(w^*\\)，就是所有的input x(data points)的linear combination：


$$
w^* = \sum_n \alpha^*_n x^n
$$


一般用Lagrange Multiplier Method可以证明，也可以通过gradient descent的过程来证明。



假设\\(w\\)有\\(k\\)维：


$$
\begin{cases}
w_1 \gets w_1 - \eta \sum_n c^n(w)x^n_1 \\
\vdots   \\
w_i \gets w_i - \eta \sum_n c^n(w)x^n_i \\
\vdots  \\
w_k \gets w_k - \eta \sum_n c^n(w)x^n_k
\end{cases}
$$


每一行的式子中，唯一不一样的地方就在最后乘上去的\\(x^n_i\\)。



**将上式的每一个w和x都串成vector，整个式子可以写成：**
$$
w \gets w- \eta \sum_n c^n(w)x^n
$$
假设现在的w被initialized成一个zero vector，那么之后的每一次update参数的时候，都是加上了data points的linear combination。反复循环递归，最终的w也可以看成是data points的linear combination。



其中，


$$
c^n(w) = \frac{\partial l(f(x^n),\hat{y}^n)}{\partial f(x^n)}
$$
这里的loss function用的是hinge loss，和RELU很像，有两个operation region。在这里如果是作用在max=0的region的话，**那么\\(c^n(w)\\)就会是0 。**



在使用hinge loss的时候，\\(c^n(w)\\)这一项常常是0，所以\\(- \eta \sum_n c^n(w) x^n\\)这一项也常常是0，导致不是每一个data point都会被加入到\\(w\\)中去。



所以最后解出来的\\(w^*\\)中的linear combination的weight(\\(\alpha^*_n\\))可能会是sparse的。



就是说可能会有很多的data points对应的\\(\alpha^*_n\\)的值等于0，**而那些值不等于0的\\(\alpha^*_n\\)对应的\\(x^n\\)就是support vectors**



\\(\alpha^*_n\\)为0所对应的\\(x^n\\)对于整个model没有什么影响力。



所以SVM相较于其他方法是比较robust。也就是如果把那些不是support vector的data points(可以是很多的outlier)移除掉，对于整个Model几乎没有什么影响。



------

将w写成是data points的linear combination：


$$
w = \sum_n \alpha_n x^n
$$
一个最大的优点是可以用上kernel trick。







