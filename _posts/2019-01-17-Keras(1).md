---
tag: 机器学习
---





# "Hello world" of Keras

Keras 的安装参照[Keras官方文档](https://keras.io/)或者是[Keras中文文档](https://keras.io/zh/)





## 下面以手写数字辨识为例子来熟悉一下Keras 的基本操作



**输入**：手写图片 28*28pixels



**输出**：0-9之间的最大几率的数字





首先设计一个Model



![01](https://ws1.sinaimg.cn/large/8f3e11fcly1fz9rzqtua3j20qd0jm0wt.jpg)

假设这个Model有两个Hidden layer ，分别都有500个neuron



**1.首先声明一个model，并假设这个Model是顺序模型**

```
model = Sequential()
```



**2.然后可以不断地堆叠层**





```
//对于第一层隐藏层来说
model.add( Dense( input_dim=28*28,output_dim=500 ))//Dense表示全连接层，可以选用其他的。28*28和500表示输入输出的维度信息dimension。
model.add( Activation('sigmoid') ) //设置activation function


```





```
//对于第二层隐藏层来说
model.add( Dense( output_dim=500 ) ) //这里不需要有input,因为第二层的input就是第一层的output
model.add( Activation( 'sigmoid' ) )
```





```
//对于输出层
model.add( Dense(ouput_dim=10 ))//output 的dimension 是10维的
model.add( Activation('softmax')) //这里选择softmax
```





**3.Goodness of Function ，来定义Model 的好坏**



**4.选出最佳Function**





![02](https://ws2.sinaimg.cn/large/8f3e11fcly1fz9s016jpdj20ni0hd419.jpg)

![03](https://wx3.sinaimg.cn/large/8f3e11fcly1fz9s0a6p04j20nm0h2dil.jpg)





```
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=[accuracy'])

//categorical_crossentropy就是交叉熵
//optimizer选择adam
//其他的优化方法例如 SGD,RMSprop,Adagrad,Adadelta,Adam,Adamax,Nadam

```



```
model.fit(x_train,y_train,batch_size=100,nb_epoch=20)
//这个函数里面有4个参数，分别为：训练集，\hat{y},每次训练（每次更新参数）随机选择的样本数（样本全部选择完毕算一轮），训练轮数
//假设有10000个样本，Keras会自动随机分配100个batch，每一个batch更新一次参数，100个batch全部更新完一次参数就是一个 epoch，所以这里会更新100*20=2000次参数
```



**在做deep learning 的时候，并不会去minimize total loss,而是去minimize 每个 batch 的 total loss，每计算一次total loss就更新一次参数**



如果 batch_size=1，就相当于是**Stochastic gradient descent**









![04](https://ws3.sinaimg.cn/large/8f3e11fcly1fz9s0kxhi0j20nu0hkn4z.jpg)





**x_train和y_train的存法**



1. x_train 是一个 numpy  array , 是一个二维数组，第一个维度表示有多少个example，例如有10000个样本就是10000行；第二个维度表示的是每张图片，例如一张图片是28*28 pixels的，就有784列。
2. y_train 的第一个维度表示有多少个examples，第二个维度用1来表示某一个选项，其余的为0。







### Batch_size 会影响到 速度和 效果（performance），需要自己去调节（tune it )。



![05](https://ws4.sinaimg.cn/large/8f3e11fcly1fz9s0wr3vvj20ne0h8gok.jpg)



例如有50000个examples



1. 如果设置batch_size=1,在每一个epoch 里面会更新50000次参数
2. 如果设置batch_size=10,在每一个epoch里面会更新5000次参数



每个epoch里面的运算量几乎是一样多的，但是在实际上batch_size设置不一样的话运算时间是不一样的。batch_size越大，每一个epoch运算的时间越小。



在同一段时间内，batch_size大和小，更新参数的次数几乎是一样的。在这里，更倾向于设置 batch_size = 10。这样在不损失速度的情况下会更稳定。



**速度快的原因是使用了并行运算**，因为整个运算可以看成是矩阵运算。

![06](https://ws3.sinaimg.cn/large/8f3e11fcly1fz9s1236btj20nj0gujsw.jpg)



如图所示



当batch_size=1时（相当于是 Stochastic Gradient Descent)，第一次取一个样本（黄色），计算\\(z^1\\)，第二次再取一个样本（绿色），计算\\(z^1\\)。



当batch_size=2时，两个样本可以拼接成一个，直接进行一次矩阵运算就能得到结果。



所以，实际上由于GPU的并行运算能力，矩阵中的每个element可以同时运算，batch_size=2时候的运算速度会比较快。



![08](https://wx4.sinaimg.cn/large/8f3e11fcly1fz9s1suydsj20mz0fvjsh.jpg)





**既然batch_size越大越稳定但是不能设置太大的原因**

* 受限于硬件条件（GPU无法并行运算了）
* batch_size太大容易陷入 local minima。而较小的batch_size有随机性，可以跳出local minima







![07](https://ws1.sinaimg.cn/large/8f3e11fcly1fz9s17kuxpj20nn0hhn05.jpg)



Keras可以保存并在下次加载model。在Documents中有说明。



也可以帮忙做testing



```
score=model.evaluate(x_test,y_test)    //有两个参数
print('Total loss on Testing Set:', score[0])
print('Accuray of Testing Set:', score[1])
```



也可以帮忙做预测（只有一个参数）

```
result=model.predict(x_test)
```







