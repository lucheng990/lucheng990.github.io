---
tag: 机器学习
---







在做一些分类或者预测的任务中，常常需要用到Ensemble这一个trick，即对几个不同的模型（NN、lgb等各类模型）进行融合。



## Framework of Ensemble



例如，在做一个分类的任务时候，有几个不同的模型（模型之间需要具有一定的差异性）




$$
f_1(x),f_2(x),f_3(x),\cdots \cdots
$$


如果能够把这些不同的classifier集合(aggregate)起来，在很多时候能够获得更好的一个performance。





# Ensemble: Bagging(强Model)



Bias-Variance Trade-Off是在机器学习中常常会遇到的事情。线性（简单）算法常常会有一个比较高的bias（意味着对target function有更多的assumptions或者是restrictions），而非线性（复杂）算法常有一个较高的variance。好的监督学习算法既要有低的variance又要有低的bias。







Bagging方法常常用于具有高variance的模型上面，将所有的模型做平均，得到
$$
E[f^*]
$$
，这个均值常常更接近于\\(\hat{y}\\)。这样就达到了一个降低variance的结果。






Bagging Ensemble的基本思想是在复杂的数据集上构建更高级的模型来减少variance，具体来说，bagging方法首先要从原数据集上面sample出\\(N'\\)个数据来代替原数据集，这里的\\(N'\\)可以就是等于N。（可以反复抽取同一个数据）。这样模型训练出来的模型和在原数据集上面训练出来的会有所不同。







分别sample出4个data sets，训练4个模型



![a1](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/23Ensemble/a1.png)



在预测的时候也分别投到4个不同的模型中，采用Average或者Voting的方法得到最终的结果。



![a2](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/23Ensemble/a2.png)







这里要注意的是：**Bagging的目的是为了decrease variance，只有当model很复杂有可能会overfitting的时候，才可以选择做bagging。**





---

### Random Forest



Random Forest主要使用了两个trick：

1. 使用了bagging ensemble
2. 在每次要产生decision tree的branch的时候（每次split时）都randomly决定哪些特征和问题是不能用的。



---



### Out-of-bag validation for bagging



bagging方法有一个validation trick：对于每一个model，分别使用没有被抽到的样本作为validation test data。最后的模型当中有一个Out-of-bag(OOB) error。



---









# Ensemble: Boosting



boosting算法常常用于一些比较弱的model中。



基本的算法思想和框架如下图：







![a3](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/23Ensemble/a3.png)





这里要注意的是：普通的Boosting无法并行训练各个model，只能sequentially训练各个模型。





## 得到各个classifiers的方法





要得到不同的model可以通过使用不同的data sets，而要获得不同的data sets可以有以下方法：







* Re-sampling data(类似bagging ensemble)









* Re-weighting data



可以给training data一些不同的weight，通过改变这个不同的weight(**作用于loss function**)来得到不同的data sets。




$$
\begin{matrix}
 (x^1,\hat{y}^1) \\
 (x^2,\hat{y}^2) \\
 (x^3,\hat{y}^3) 
\end{matrix}
\qquad \Longrightarrow 
\qquad
\begin{align*}
& (x^1,\hat{y}^1,u^1) \\
& (x^2,\hat{y}^2,u^2) \\
& (x^3,\hat{y}^3,u^3)
\end{align*}
$$




在给每一笔data加上一个weight以后，在loss function中可以进行修改：






$$
L(f) = \sum_n l(f(x^n),\hat{y}^n) \\
\Downarrow \\
L(f) = \sum_n u^n l(f(x^n),\hat{y}^n)
$$


这样就能够训练出不同的Model出来了。





# Adaboost







Adaboost的基本思想是：先训练好一个classifier \\(f_1(x)\\)，现在要找一组新的training set来训练第二个classifer \\(f_2(x)\\)。而这一组新的training set就是会让\\(f_1(x)\\) 预测错误的data。实际中详细的做法如下：









记\\(\varepsilon_1\\)为error rate of \\(f_1(x)\\) on its training data，即，




$$
\varepsilon_1 = \frac{\sum_n u^n_1 \delta(f_1(x^n) \ne \hat{y}^n)}{Z_1} \qquad , \quad Z_1 = \sum_n u^n_1 \tag{0}
$$




在上面的式子中的\\(\delta\\)函数中，如果training example中的结果是对的就是0，错的就是1；\\(u^n_1\\)为weight，因为weight的值合起来不一定是1，所以这里要做一下normalization，所以要除以分母。





\\(\varepsilon\\)一定会小于0.5，因为在这个二分类的问题当中，NN不会让错误率大于0.5，一旦大于0.5把output反过来就是小于0.5。





**为了得到新的training set，下一步要做的事情是，改变weights from \\(u^n_1\\) to \\(u^n_2\\) such that：**


$$
\frac{\sum_n u^n_2 \delta(f_1(x^n) \ne \hat{y}^n)}{Z_2}  = 0.5 \tag{1}
$$




也就是说，现在要改变(0)式当中的weights \\(u\\)，能够使得(0)式的值变成0.5，这个0.5其实就是和随机random的结果一样了，也是一个最差的结果（因为不可能大于0.5，上面解释过）。这样就可以得到一组新的weights，将这组新的weights应用到data上面，就可以得到一组新的training set。





在新的weights \\(u^n_2\\)上面就可以训练第二个model \\(f_2(x)\\)。





---



> 举一个例子





现在有4笔training data，假设初始状态下weights全为1:


$$
\begin{align*}
& (x^1,\hat{y}^1,u^1) \qquad u^1 = 1  \tag{a}\\
& (x^2,\hat{y}^2,u^2) \qquad u^2 = 1  \tag{b}\\
& (x^3,\hat{y}^3,u^3) \qquad u^3 = 1  \tag{c}\\
& (x^4,\hat{y}^4,u^4) \qquad u^4 = 1  \tag{d}
\end{align*}
$$






在这些data上面训练一个model \\(f_1(x)\\)，假设这个model在acd上面的结果是正确的，b中的结果是错误的。所以，




$$
\varepsilon_1 = 0.25
$$




现在要改变data的weight，让\\(\varepsilon_1 = 0.25 \\)。显然改法有很多种，但是有一个大的方向，就是要提高结果错误的data weight，降低正确的data weight。例如，可以修改成：






$$
\begin{align*}
& (x^1,\hat{y}^1,u^1) \qquad u^1 = 1/ \sqrt{3}  \tag{a}\\
& (x^2,\hat{y}^2,u^2) \qquad u^2 = \sqrt{3}  \tag{b}\\
& (x^3,\hat{y}^3,u^3) \qquad u^3 = 1/ \sqrt{3}  \tag{c}\\
& (x^4,\hat{y}^4,u^4) \qquad u^4 = 1/ \sqrt{3}  \tag{d}
\end{align*}
$$




对acd（correctly classified）上面的weight除上\\(d_1\\)，对b（misclassified）上面的weight乘上\\(d_1\\)。得到一组新的training data，在这组training data上面可以训练第二个classifier\\(f_2(x)\\)。





最后第二个classifier可以得到的\\(\varepsilon_2\\)是小于0.5的。







---





总结一下，得到新的weights的过程如下图，







![a4](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/23Ensemble/a4.png)







---

下面对weight的改变做一个推导(求\\(d_1\\))。



现在已经算出了\\(\varepsilon_1\\)：


$$
\varepsilon_1 = \frac{\sum_n u^n_1 \delta(f_1(x^n) \ne \hat{y}^n)}{Z_1} \qquad Z_1 = \sum_n u^n_1
$$



现在的目标是要找到一组新的weight，让上式等于0.5，即：



$$
\begin{align*}
 if \qquad &f_1(x^n) \ne \hat{y}^n, \quad u^n_2 \gets u^n_1 \cdot d_1 \\
else \; if \qquad & f_1(x^n) = \hat{y}^n,   \quad u^n_2 \gets \frac{u^n_1}{d_1}
\end{align*}
$$





在这之后，让：


$$
\varepsilon_1 = \frac{\sum_n {\color{red} u^n_2} \delta(f_1(x^n) \ne \hat{y}^n)}{Z_2} = 0.5
$$





对于\\(\varepsilon_1\\)的分子的部分，因为正确的部分的\\(\delta\\)为0，上式的分子：




$$
\begin{align*}
& \sum_{n} {\color{red} u^n_2} \delta(f_1(x^n) \ne \hat{y}^n) \\
= & \sum_{f_1(x^n) \ne \hat{y}^n} u_1^n d_1
\end{align*}
$$


而分母部分：




$$
\begin{align*}
  Z_2 = & \sum_n u^n_2 \\
  = & \sum_{f_1(x^n) \ne \hat{y}^n} u^n_2 + \sum_{f_1(x^n) = \hat{y}^n} u^n_2 \\
  = & \sum_{f_1(x^n) \ne \hat{y}^n} u^n_1 d_1 + \sum_{f_1(x^n) = \hat{y}^n} \frac{u^n_1}{d_1}
 
\end{align*}
$$




所以，原式就变成了下面这个式子，要求是让它等于0.5。






$$
\frac{\sum_{f_1(x^n) \ne \hat{y}^n} u_1^n d_1}{\sum_{f_1(x^n) \ne \hat{y}^n} u^n_1 d_1 + \sum_{f_1(x^n) = \hat{y}^n} \frac{u^n_1}{d_1}} = 0.5
$$




分子分母换个位置：




$$
\frac{\sum_{f_1(x^n) \ne \hat{y}^n} u^n_1 d_1 + \sum_{f_1(x^n) = \hat{y}^n} \frac{u^n_1}{d_1}}{\sum_{f_1(x^n) \ne \hat{y}^n} u_1^n d_1} = 2
$$




约去通项，移项，得到：




$$
\sum_{f_1(x^n) = \hat{y}^n} \frac{u^n_1}{d_1} = \sum_{f_1(x^n) \ne \hat{y}^n} u_1^n d_1
$$




将\\(d_1\\)提出去，得到：




$$
\frac{1}{d_1} \sum_{f_1(x^n) = \hat{y}^n} u^n_1 = d_1 \sum_{f_1(x^n) \ne \hat{y}^n} u^n_1 \tag{2}
$$




回到最初的\\(\varepsilon_1\\)的式子：




$$
\varepsilon_1 = \frac{\sum_n u^n_1 \delta(f_1(x^n) \ne \hat{y}^n)}{Z_1}  = \frac{\sum_{f_1(x^n)\ne \hat{y}^n} u_1^n}{Z_1}
$$


所以，




$$
\sum_{f_1(x^n) = \hat{y}^n} u^n_1  = Z_1 \varepsilon_1
$$




所以，(2)式就变成了：




$$
\begin{align*}
& Z_1(1-\varepsilon_1)/d_1 = Z_1 \varepsilon_1 d_1 \\
\Longrightarrow & d_1 = \sqrt{(1-\varepsilon_1)/ \varepsilon_1}
\end{align*}
$$


这样就得到了\\(d_1\\)，它的值一定大于1。





---





## Algorithm for AdaBoost





![a5](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/23Ensemble/a5.png)





这里有几个点。



* 初始的weight设置为1
* T个iteration，每个iteration都分别得到1个弱classifier（通过不同的权重的data sets\\(\{u^1_t, \cdots, \u^N_t\}\\)
* 权重的设置按照上面的规则，错误分类的样本乘以\\(d_t\\)来扩大权重，正确的样本除以\\(d_t\\)。
* \\(d_t\\)的设置可以是




$$
d_t = \sqrt{(1-\varepsilon_t)/\varepsilon_t}
$$




* 为了表达式的同一，可以加一个ln：




$$
\alpha_t = \ln{\sqrt{(1-\varepsilon_t)/\varepsilon_t}}
$$




对于这种写法，可以得到权重的更新：





**错误分类的样本**




$$
u^n_{t+1} = u^n_t \times d_t  = u^n_t \times \exp{(\alpha_t)}
$$




**正确分类的样本**




$$
u^n_{t+1} =  u^n_t / d_t = u^n_t \times \exp{(-\alpha_t)}
$$






**结合一下，类似SVM中的loss function的写法，得到data sets的weights更新:**






$$
u^n_{t+1} \gets u^n_t \times \exp{(-\hat{y}^nf_t(x^n)\alpha_t)}
$$






### Aggregate所有model的output







当训练得到各个model：




$$
f_1(x), \cdots , f_t(x), \cdots,f_T(x)
$$




以后，需要对结果做一个汇总，办法有：







* Uniform weight：




$$
H(x) = \mathrm{sign} (\sum^T_{t=1} f_t(x))
$$




\\(H(x)\\)大于或者小于0对应了两个不同的类。这样会存在一个问题：各个弱分类器之间也存在强弱，直接求和会存在被弱弱分类器影响的结果。





* Non-uniform weight:




$$
H(x) = \mathrm{sign} (\sum^T_{t=1} {\color{red}\alpha_t}f_t(x))
$$




其中，权重项和上面的data sets上面的更新的权重项一样，即，




$$
{\color{red} \alpha_t} = \ln{\sqrt{(1-\varepsilon_t)/\varepsilon_t}}
$$








对不同的弱分类器分别加了一个权重，越大的错误率\\(\varepsilon_t\\)具有越小的权重\\({\color{red} \alpha_t}\\)。







---



## 证明：Adaboost有更好的performance









Adaboost的效果会随着Iteration的增多（弱models的增加）越来越好。下面来证明这件事情：





使用Adaboost以后，得到最终的classifier：




$$
H(x) = \mathrm{sign} (\underbrace{\sum^T_{t=1} \alpha_t f_t(x)}_{{\color{red} g(x)}}) \qquad , \quad \alpha_t = \ln{\sqrt{(1-\varepsilon_t)/\varepsilon_t}}
$$



记括起来的部分为\\(g(x)\\)。



最终的Error Rate:




$$
\frac{1}{N} \sum_n \delta(H(x^n) \ne \hat{y}^n)
$$





也可以写成：




$$
\frac{1}{N}\sum_n \delta(\hat{y}^n g(x^n)<0)
$$


因为小于0说明预测结果和实际结果不一致。







这一项Error Rate有一个**Upper bound**（对于每一个n的取值来说，这里的\\(\delta\\)不会大于1，确保小于等于恒成立加一个负号。），如下：






$$
\frac{1}{N}\sum_n \delta(\hat{y}^n g(x^n)<0) \leq \frac{1}{N} \sum_n \exp{(-\hat{y}^n g(x^n))} =\frac{1}{N}Z_{T+1}
$$




最后的结果等于\\(Z_{T+1}\\)的原因下面解释。





![a6](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/23Ensemble/a6.png)



蓝色的线即为upper bound。







### 现在就是要证明这个upper bound会越来越小





在证明之前，需要来求另外一个数\\(Z_t\)，也就是第t个iteration的时候给training data的weights的summation。









当iteration = 1的时候，




$$
u^n_1 = 1 \tag{10}
$$










而后一个\\(u\\)都是由上一次的更新得到：






$$
u^n_{t+1} = u^n_t \times \exp{(-\hat{y}^n f_t(x^n) \alpha_t)} \tag{11}
$$






结合（10）和（11），得到：






$$
u^n_{T+1}  = \prod_{t=1}^T \exp{(-\hat{y}^nf_t(x^n)\alpha_t)}
$$










所以，对于\\(Z_{T+1}\\)，即当第T+1个iteration的时候，




$$
Z_{T+1} = \sum_n  u^n_{T+1}= \sum_n \prod_{t=1}^T \exp{(-\hat{y}^nf_t(x^n)\alpha_t)}
$$




因为指数相乘等于次数相加，所以可以把这个连乘放到指数的次数项中：




$$
Z_{t+1} = \sum_n \exp{(-\hat{y}^n \underbrace{\sum_{t=1}^T f_t(x^n)\alpha_t}_{\color{red}g(x)})}
$$




这个括起来的部分就是上面的g(x)。





可以发现，exponential里面的项就是上面的**Upper bound**的一部分。Upper bound:





$$
\frac{1}{N} \sum_n \exp{(-\hat{y}^n g(x^n))} =\frac{1}{N}Z_{T+1}
$$







**所以，要证明Performance会随着t的增加而变好，就是要证明weight的summation会越来越小。**







初始权重为1，所以：




$$
Z_1 = N
$$






而，




$$
Z_t = {\color{blue} Z_{t-1}\varepsilon_t }\exp{(\alpha_t)}  + {\color{blue} Z_{t-1} (1-\varepsilon_t)}\exp{(-\alpha_t)} \tag{12}
$$




其中，\\(Z_{t-1} \varepsilon_t\\)为分类错误的portion in \\(Z_{t-1}\\)，后一项蓝色的为分类正确的。





因为，




$$
\alpha_t = \ln{\sqrt{(1-\varepsilon_t)/\varepsilon_t}}
$$




所以，(12)式又等于：




$$
Z_t = Z_{t-1} \varepsilon_t \sqrt{(1-\varepsilon_t)/\varepsilon_t} +Z_{t-1} (1-\varepsilon_t) \sqrt{\varepsilon_t / (1-\varepsilon_t)}
$$









约掉分母，合并得到：




$$
Z_t = Z_{t-1} \times 2\sqrt{\varepsilon_t(1-\varepsilon_t)}
$$




从这里可以看出\\(Z_t\\)是单调递减的，写出递推式：




$$
Z_{T+1} = N \cdot \prod_{t=1}^T \cdot 2\sqrt{\varepsilon_t(1-\varepsilon_t)}
$$




所以upper bound会越来越小，也就是error rate会越来越小。







---











**在Adaboost的实际训练中，当train data上面的error rate为0的时候，此时如果继续训练，那么在test data上面的表现还会继续变好。**





---





# Gradient Boosting





Gradient Boosting是Adaboost更general的算法。



**整个Algorithm框架如下：**





![a7](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/23Ensemble/a7.png)







记：






$$
g_t = \sum_{i=1}^{t} \alpha_i f_i(x)
$$






和Adaboost类似，\\(\alpha\\)是weight，\\(f_i(x)\\)是一些weak classifier。













首先需要有一个classifier \\(g_0(x)\\)，然后，对于每一个iteration，已知的是：




$$
g_{t-1}(x) = \sum_{i=1}^{t-1} \alpha_i f_i(x)
$$








接下来的目的是要找到一个\\(f_t(x)\\)和\\(\alpha_t\\)，来弥补\\(g_{t-1}(x)\\)，然后得到新的\\(g(x)\\)：




$$
g_t(x) = g_{t-1}(x) + \alpha_t f_t(x)
$$





结束T个iteration，最终的模型：






$$
H(x) = \mathrm{sign} (g_T(x))
$$



---











所以，GB的关键在找到T个iteration中的\\(f_t(x)\\)和\\(\alpha_t\\)。因为这里是要找一个NN(或者其他function)，三步骤的第二步，需要有一个objective function或者是损失函数。





**Objective function:**




$$
L(g) = \sum_n l(\hat{y}^n,g(x^n))
$$


这里可以选择各种Loss function L。假设L是一个exponential函数，目的是希望两个变量尽量同号，而且同号相乘越大越好，加个负号可以改成去minimize它：






$$
L(g) = \sum_n \exp{(-\hat{y}^ng(x^n))}
$$







现在已知了\\(g_{t-1}\\)，如果使用Gradient Descent去optimize：





$$
g_t \gets g_{t-1} - \nabla \frac{\partial L(g)}{\partial g(x)} \bigg|_{g(x) = g_{t-1}(x)} \tag{30}
$$









这里是一个函数对L做微分，这件事情是可以做的，分母同时在分子里面出现并作为分子的一部分可以把整个函数看成一个变量。先不考虑learning rate，进行求微分，得到：




$$
-\sum_n \exp{(-\hat{y}^ng_{t-1}(x^n))(-\hat{y}^n)}   =\sum_n \exp{(-\hat{y}^ng_{t-1}(x^n))(\hat{y}^n)} \tag{31}
$$










从boost的角度来看，要求：




$$
g_t(x) = g_{t-1}(x) + \alpha_t f_t(x) \tag{32}
$$






如果（30）和（32）的第二项具有相同的方向。那么同样也能够起到minimize loss的作用。这件事情是比较intuition的。也就是说，现在要**找一个**\\(f_t(x)\\)，能够和（30）的后半项，也就是这个求出来的微分（31）的方向越一致越好。这一点也可以转换成为，要求这两项的乘积，而且这个乘积越大越好，写出乘积：




$$
\sum_n \exp{(-\hat{y}^ng_{t-1}(x^n)) \cdot (\hat{y}^n)} \cdot f_t(x) \tag{33}
$$




现在就是要找一个\\(f_t(x)\\)，能够让（33）式越大越好。也就是要让每一笔data的\\(\hat{y}^n\\)和\\(f_t(x)\\)都具有相同的符号，然后第一个因子\\(-\hat{y}^ng_{t-1}(x^n)\\)可以看成是一个weight，写成\\(u^n_t\\)。





对于这里的weight，\\(u^n_t\\)，：


$$
u^n_t = \exp{(-\hat{y}^ng_{t-1}(x^n))}
$$


因为,




$$
g_{t-1}(x) = \sum_{i=1}^{t-1} \alpha_i f_i(x)
$$






所以，\\(u^n_t\\)可以写成：




$$
\exp{(-\hat{y}^n\sum_{i=1}^{t-1} \alpha_i f_i(x))}
$$




exp相加的变成连乘：




$$
u^n_t = \prod_{i=1}^{t-1}  \exp(-\hat{y}^n \alpha_i f_i (x^n))
$$



这里的weight \\(u^n_t\\)，exactly is weight of Adaboost。就是Adaboost里面的weight。









所以，根据






$$
g_t(x) = g_{t-1}(x) + \alpha_t f_t(x)
$$




这里面要找的\\(f_t(x)\\)就是adaboost里面的weak classifier \\(f_t(x)\\)，而\\(\alpha\\)则是一个类似learning rate的东西。





有了\\(f_t(x)\\)，还需要找到一个\\(\alpha_t\\)。这个类似learning rate的值可以intuitive的设置一个固定值。但是为了减少运算量（算出\\(f_t(x)\\)本身需要很大的运算量），设置\\(\alpha_t\\)的值可以有几个trick。





Gradient Boosting的做法是：固定(fix)住\\(f_t(x)\\)，然后去找一个\\(\alpha_t\\)，能够让loss function L最小。也就是要找到一个\\(\alpha_t\\)，让：




$$
\frac{\partial L(g)}{\partial \alpha_t} = 0
$$




如下图所示：





![a8](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/23Ensemble/a8.png)





结果找出来的值就是:




$$
\alpha_t = \ln{\sqrt{(1-\varepsilon_t)/\varepsilon_t}}
$$








也是Adaboost里面的这个weight。



**所以Adaboost也可以看成是一种特殊的Gradient Boosting。**





而Gradient Boosting的一个优势是，可以任意更改Objective Function(在上文加粗显示的地方)，从而创造出不一样的Boosting的方法。













# Ensemble: Stacking





Stacking也是一种Ensemble的方法。







假设现在已经有了4个model，要让结果再提升的话，给定一个input x，可以把4个model的output结合起来，例如vote投票的方法来决定最终的output。









但是，如果简单地这样做会存在一个问题，有的model的performance会弱一点。





Stacking的方法是：**再另外train一个Final Classifier，将4个model的output作为input feature，这个Final Classifier的output才是最终的output。**





这里有一个**trick**，将原来的training data分成两个部分，一个部分用来训练前面4个model，另一部分用来训练这个Final Classifier。











