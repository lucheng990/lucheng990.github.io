---
tag: 机器学习
---







# Recurrent neural network



传统的NN很难去解决一些输入是上下文之间有关联（temporal dynamic behavior）的问题。而RNN可以通过其internal state(memory)来解决这一类的问题。





如下图所示，是一个最简单的RNN模型：

![a1](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/25RNN/a1.png)





其中的各个神经元和以前一样，神经元有输入权重、bias，激活函数等，只是第一个隐藏层的输入多了一个memory的输入，这个memory是前一次中的隐藏层的输出。





RNN形式可以很灵活，其一体现在整个模型的输入和输出的长度上，例如，上图的就是两（多）个输入两（多）个输出的RNN，还可以有一个输入多个输出、多个输入一个输出之类的情况，具体要根据所做的任务来灵活选择设计。





举一个更深的RNN的例子，如下图：



![a2](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/25RNN/a2.png)







RNN有很多的变种，较早提出的，又被称为SRN(simple recurrent networks)有Elman networks和Jordan networks。[Wiki链接](https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks)





![a3](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/25RNN/a3.png)





如上图所示，其Context units分别fed from hidden layer和output layer。





* Bidirectional RNN



假设input一个句子，可以同时从两个方向分别训练两个rnn（可以合并到一个NN一起训练），然后分别拿出其中一层hidden layer的output接到一起作为输出。这样的好处是每次产生一个y都可以看到整个input。



![a4](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/25RNN/a4.png)







---



单纯的RNN在梯度反向传播的时候因为无法处理随着递归，**权重指数级爆炸**或者**梯度消失**问题，难以捕捉长期时间关联，我在后面的一篇文章中写了证明过程，而结合不同的**LTSM**可以很好解决这个问题。





## LSTM





LSTM适合于处理和预测时间序列中间隔和延迟非常长的重要事件，其一般结构如下图所示：



![a5](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/25RNN/a5.png)



一共有4个部分输入，上图从上往下分别为：Output gate、Forget gate、Input gate以及一个原始信息的输入。在每一个时间段中，memory的值会被覆盖一次。



**输入的矩阵是原来的\\(x^t\\)和上一个时间段中该LSTM单元的output（通常LSTM单元是属于隐藏层部分，所以又可以说是hidden layer output）并在一起，作为输入。**用公式写出来就是：






$$
\begin{align*}
 & f_t = \sigma_g(W_fx_t + U_fh_{t-1} + b_f) \\
 & i_t = \sigma_g(W_ix_t+U_ih_{t-1}+b_i) \\
 & o_t = \sigma_g(W_ox_t + U_oh_{t-1} +b_c) \\
 & \widetilde{c}_t = \sigma_h(W_cx_t + U_ch_{t-1} + b_c)\\
 & c_t = f_t \otimes c_{t-1} + i_t \otimes \widetilde{c}_t \\
 & h_t = o_t \otimes \sigma_h(c_t)


\end{align*}
$$



其中，初始值\\(c_0\\)=0，\\(h_0\\)=0；\\(\otimes\\)表示element-wise product；下标t表示一个时间段。W，U，b表示要训练的参数。f,i,o分别表示三个门的输入activation function。



另外，



* \\(x_t\\)表示输入的data x
* \\(h_t\\)表示（上一个的）隐藏层输出，或者叫LSTM的output
* \\(\widetilde{c}_t\\)表示输入的data x然后要经过的activation function
* \\(c_t\\)表示memory中的存放的向量
* \\(\sigma_g\\)表示sigmoid function，取值0-1之间，非常适合表示gate的开关状态
* \\(\sigma_c\\)表示双曲正切（hyperbolic tangent function)函数
* \\(\sigma_h\\)表示hyperbolic tangent function，在peephole LSTM的论文中，这里suggest是\\(\sigma_h(x)=x\\)





## Peephole LSTM



在Peephole LSTM中，对上面的LSTM做了一个小变形，在大部分的情况下，使用的输入是memory和data x的组合而不是hidden layer output和data x的组合。wiki截图：





![a6](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/25RNN/a6.png)





![a7](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/25RNN/a7.png)



上上图的小圆圈乘法和前面写的小圆圈中间一个×是一样的，都是指element-wise product。





---



在实际应用中，LSTM的数量会比较多，层数也会比较多，下面的图显示了LSTM layers之间是如何串联的。



![a705](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/25RNN/a705.png)



![a8](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic/25RNN/a8.png)



注意，上图每一层左右两个LSTM单元之间是时间上的联系。





---

参考资料和视频：



<iframe width="717" height="538" src="https://www.youtube.com/embed/xCGidAeyS4M?list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>





<iframe width="956" height="538" src="https://www.youtube.com/embed/WCUNPb-5EYI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>





[長短期記憶]([https://zh.wikipedia.org/wiki/%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6](https://zh.wikipedia.org/wiki/長短期記憶))

[Long_short-term_memory](https://en.wikipedia.org/wiki/Long_short-term_memory)
