---
tag: 机器学习
---







**Unsupervised learning可以做的事情**



![a1p2](https://wx1.sinaimg.cn/large/8f3e11fcly1g22hk46rocj20nn0gydl3.jpg)


* 化繁为简（Clustering和Dimension Reduction共同使用）


如上图左，假设有很多种不同的input，现在要找一个function。使得input的那些看起来像是树的东西的output都是一棵树。

在这个例子中，做unsupervised learning的时候，只会有function的其中一边（不知道label, output是什么）

这里有两点：

1. Clustering，将这些input（unlabeled）归类。
2. Dimension Reduction：提取主要特征，减小数据量。将比较复杂的input变成比较简单的output。


* 无中生有(Generation)

**??**

如上图右，现在目的是要找一个function，随机给它一个Input，例如数字1，就能output（画出）一棵树；给它另一个input，例如数字2，就能output另外一棵树。


现在要找到这个可以画图的function，但是只知道这些图片(output)，没有input，不知道要输入怎么样的数字才得到。



# Clustering


> 假设现在要做image clustering



有一大堆的images，现在要把它们分类。也就是要把有些不同的image贴上同一个标签。


![a2p5](https://ws1.sinaimg.cn/large/8f3e11fcly1g22hkeexf4j20nt0hsgrd.jpg)


最critical的问题是：需要有多少个clusters？


选择适当的clusters数需要empirical地来决定。




## K-means

在clustering中，最常用的方法是K-means。做法如下。


* 假设现在有一大堆unlabeled data，上标分别从1到N。现在要做的事情是要将他们归为K类（即有K个Cluster）。
* 首先初始化(random)K个中心。从training data中随机找出K个object出来，作为中心。
* 重复地更新参数：
    * 对所有training data中的x，取一个binary的(0和1)值。假设某个object \\(x^n\\)和\\(c^i\\)，第i个center最为接近，那么\\(b^n_i\\)就为1，否则为0 。
    * update cluster(假设要更新第i个center)：把所有属于第i个cluster的object全部拿出来，做平均。就得到新的中心。
    * 重复。

～～显示是否正常～～


这里要注意：第一次初始化中心的时候，如果纯粹随机，而不是从data points中间来挑选，有可能会出现没有任何一个example和某个cluster很像，这样就不能够更新参数。




## HAC(Hierarchical Agglomerative Clustering)


有的时候，如果不容易确定cluster的数目。可以用HAC的办法。做法如下：


* 先建一个tree，类似哈夫曼树。

> e.g. 假设有5个example


对这5个example分别两两去算相似度（方法有很多）。挑出最相似的pair，并对其merge起来，可以取平均。得到一个新的vector，这个vector同时代表这两个objects。


现在就变成有4个objects了。两两计算相似度，merge最相似的pair。可以取平均，又得到一个新的vector代表这个pair。

重复进行。可以得到如下图所示的树。

![a3p6](https://ws1.sinaimg.cn/large/8f3e11fcly1g22hko2qq6j20n80hcmy2.jpg)


* 选取一个threshold，直观来看就是切一刀。




越靠近root的分支表示比较不像。



要做clustering，做法是横着切一刀，如上图右。这一刀切过几条线就表示有几个cluster。比如红色一刀，就分成了两个cluster，分别是下面框起来的部分。



HAC和K-means最大的区别是可以不用直接决定Cluster的数目。而是决定要切在树的structure的哪里。但是效果都差不多。




# Distributed Representation



光只做Cluster是非常"卡"的。

Cluster是以偏概全，因为某个object都必须要属于某个cluster。

>e.g.



![a4p7](https://ws4.sinaimg.cn/large/8f3e11fcly1g22hkwqg98j20nf0h8gpb.jpg)



如上图右上所示，现在假设有一种东西具有多个系的特征，如果用cluster来做的话，也就是它必须要属于某一个特定的类（以偏概全）。这样并不符合实际。


所以应该用一个vector来表示这个object。

这个vector中的每一个dimension就代表了某一种特值(attribute)。这一件事情就叫做**Distributed representation**。



假设原来的东西是一个非常high dimensional的东西，例如image。现在用一个特值(attribute)来描述它，那么它就会从比较高纬的空间变成比较低纬的空间。这一件事情就叫做**Dimension Reduction**。





# Dimension Reduction


从另外一个角度来看Dimension Reduction


> e.g. 



![a5p8](https://wx1.sinaimg.cn/large/8f3e11fcly1g22hl4tum8j20ms0hf40e.jpg)


假设有一笔data的分布是这样的，如上图右中，是一个螺旋的样子。


如果用3D的空间来描述这些data其实是很浪费的，完全可以把它给展开摊平。只需要2D的空间就能够来描述这个3d的information。


> 另外的例子

在MNIST里面，每一个Image都是用28乘28维度来表示。但是实际上多数28乘28维度的图片（随机的点），看起来都不像是一个数字。

所以在这个28乘28维度的空间里面，是数字的vector很少。所以描述一个digit，完全不需要用到28乘28维。




## 具体做法

在做**Dimension Reduction**的时候，需要找一个function。这个function的input是一个vector x，output是另外一个vector z。


其中，z的dimension要比input x的要小。

下面讲了几种Dimension Reduction的方法。




# Feature selection


如上图。

把data的分布拿出来看一下，如果这些feature都集中在某几个特定的维度上，就可以把另外的一些给拿掉。


但是这个方法不见得一定有用，因为有很多时候任何一个dimension都不能给拿掉。




# Principle component analysis(PCA)




在PCA中，将x变成z的这个function是一个很简单的linear function。x和z之间的关系是一个linear transform。将x乘上一个matrix w，就得到了z。



$$
z=Wx
$$





**现在要做的事情，因为不知道z长什么样子，所以就是要根据一大堆的x，来把matrix W找出来。**




> 考虑一个比较简单的case(1-D)



PCA要做的事情，就是找这个w。


1维的vector就是一个scalar。这个z是1维的。那么这个W就是一个row vector。用上标1来表示是第一个row。


$$
z_1=w^1 \cdot x
$$



其中，\\(w^1\\)的模长是1 。即

$$
\lVert w^1 \rVert_2=1
$$



\\(w^1\\)和x做inner product所得到的\\(z_1\\)，意味着现在的x是高维空间中的一个点，\\(w^1\\)是现在高维空间中的一个vector。所以\\(z_1\\)就是x在\\(w^1\\)上面的一个投影，投影的值就是这个inner product。



现在将所有的x都透过\\(w^1\\)进行投影变成\\(z_1\\)，得到一堆\\(z_1\\)。**问题是：\\(w^1\\)应该是怎么样子的，应该选择哪一个\\(w^1\\)。**






![a6p9](https://ws1.sinaimg.cn/large/8f3e11fcly1g22hlfq04vj20ng0h3q5z.jpg)


如上图右上，假设这就是x的分布。要把二维投影到一维可以选择很多\\(w^1\\)，选择不同的方向所得到的project的结果都是不一样的。



要尽可能地记录保留原始数据的有效特征，希望这些所有的input x经过\\(w^1\\) product以后得到的所有\\(z_1\\)的分布越大越好，即具有最大的variance。这样在做完投影以后不同的x之间的区别仍然能够明显地看出来。





用equation来表示：



$$
\mathrm{Var}(z_1)=\frac{1}{N} \sum_{z_1}(z_1-\bar{z_1})^2
$$



现在要去maximize的对象是\\(z_1\\)。



找到一个\\(w^1\\)，能够使得\\(z_1\\)最大，那么就结束了，这个\\(w^1\\)就找出来了。





**但是有的时候不只是想要投影到一维，如果想要投影到一个二维的平面上**



这个时候就把x和另外一个\\(w^2\\)做inner product，得到\\(z_2\\)。


\\(z_1\\)和\\(z_2\\)串起来得到\\(z\\)；\\(w^1\\)和\\(w^2\\)的transpose排起来得到\\(w\\)。


$$
W= \begin{bmatrix}
(w^1)^T \\
(w^2)^T\\
\vdots
\end{bmatrix}
$$


**这里要注意的是**


* 首先，\\(w^2\\)的two norm是1，和前面一样。


$$
\lVert w^2 \rVert_2=1
$$

* 其次，我们希望得到的\\(z_2\\)的分布也是越大越好



* 最后，要保证\\(w^1\\)和\\(w^2\\)是orthogonal(正交)的，否则无意义。



**增加constraint：**


$$
w^1 \cdot w^2=0
$$



所以，最终得到的\\(W\\)是一个orthogonal matrix。







## 求解：找到w



*可以将PCA这件事情描述成一个neural network，用gradient descent方法来求解*








# PCA的数学推理


首先,\\(z_1\\)等于\\(w^1\\)和\\(x\\)的inner product。


$$
z_1=w^1 \cdot x
$$


现在来算\\(z_1\\)的平均值：summation over所有的\\(z_1\\)，也就是summation over所有的\\(w^1 \cdot x\\)。




$$
\bar{z_1}=\frac{1}{N}\sum z_1=\frac{1}{N} \sum w^1 \cdot x
$$


这里可以把\\(w^1\\)提出来，得到：


$$
w^1 \cdot \frac{1}{N} \sum x=w^1 \cdot \bar{x}
$$



现在要maximize的对象是\\(z_1\\)的variance，也就是：


$$
\mathrm{Var}(z_1) = \frac{1}{N} \sum_{z_1}(z_1-\bar{z_1})^2
$$



代入，提出\\(w^1\\)：

$$
\mathrm{Var}(z_1) = \frac{1}{N} \sum_{z_1}(z_1-\bar{z_1})^2 \\
=\frac{1}{N} \sum_x(w^1 \cdot x-w^1 \cdot \bar{x})^2 \\
=\frac{1}{N} \sum (w^1 \cdot (x-\bar{x}))^2
$$


**继续整理变化，这里要用到线代的知识：**



在上面的式子中，\\(w^1\\)和\\((x-\bar{x})\\)都是vector，且默认是列向量。



现在假设有a和b两个列向量。那么a和b的inner product就等价于a的transposition和b进行矩阵乘法。又由于进行矩阵乘法以后得到的是一个scalar，所以这里的平方可以直接展开。



$$
(a \cdot b)^2=(a^T b)^2=a^T b a^T b
$$


因为a的transposition乘上(矩阵乘法)b是一个scalar，所以对这个结果再进行transpose以后还是它自己。所以，


$$
a^T b=(a^T b)^T  \qquad , \\
a^T b a^T b=a^T b(a^T b)^T 
$$


根据穿脱原理，得到

$$
a^T b(a^T b)^T =a^T b b^T a
$$


**总结一下，**


$$
(a \cdot b)^2=a^T b b^T a
$$




回到原来的式子中去，


$$
\frac{1}{N} \sum(w^1 \cdot (x-\bar{x}))^2 = \frac{1}{N} \sum (w^1)^T(x-\bar{x})(x-\bar{x})^T w^1
$$


因为这个summation over是有关x的，与\\(w^1\\)无关，所以把和\\(w^1\\)相关的拿出去，因为矩阵乘法不能随便改变位置顺序，后面的\\(w^1\\)用小空格表示拿出去：



$$
\frac{1}{N} \sum (w^1)^T (x-\bar{x})(x-\bar{x})^T w^1 =(w^1)^T \frac{1}{N} \sum (x-\bar{x})(x-\bar{x})^T \quad w^1
$$


到这一步可以发现，
$$
\frac{1}{N} \sum (x-\bar{x})(x-\bar{x})^T 
$$
其实就是x的covariance。所以原式就变成了：


$$
(w^1)^T \frac{1}{N} \sum (x-\bar{x})(x-\bar{x})^T \quad w^1=(w^1)^T \mathrm{Cov}(x) w^1 
$$


总结一下，现在的问题是，要最大化\\(z_1\\)的covariance，也就是要找到一个\\(w^1\\)，能够最大化
$$
\mathrm{Var}(z_1)=\frac{1}{N} \sum_{z_1} (z_1-\bar{z_1})^2=(w^1)^T\mathrm{Cov}(x)w^1
$$

**而且，这里有一个constraint，也就是\\(w^1\\)的two norm要等于1**，否则取值就可以达到无穷大。


$$
\lVert w^1 \rVert_2=(w^1)^Tw^1=1
$$



**有了这些以后，接下来就要解这个optimization problem**



记

$$
S=\mathrm{Cov}(x)
$$



这里的这个S是Symmetric matrix，同时也是positive-semidefinite。（也就是所有的eigenvalues都是non-negative的）


**解法可以用拉格朗日数乘法(Lagrange multiplier)**


列一个式子


$$
g(w^1)=(w^1)^T S w^1-\alpha ((w^1)^T w^1 -1)
$$




同时，让g对所有的w求偏微分：





$$
\begin{cases}
\partial g(w^1) / \partial w^1_1 =0 \\
\partial g(w^1) / \partial w^1_2=0 \\
\qquad   \quad \vdots
\end{cases}
$$




其中，\\(w\\)是一个vector，里面有很多elements，\\(w^1_1\\)，\\(w^1_2\\)，……。


让这些偏微分统统等于0.




**整理以后，得到，**



$$
S w^1 -\alpha w^1 =0 
$$




也就是



$$
S w^1=\alpha w^1
$$


其中，S是一个matrix，\\(\alpha\\)是一个scalar。满足这个式子，\\(w^1\\)就是S的一个eigenvector。


**现在的问题是，**S的eigenvector有一大把，并且满足two-norm都是1 。


所以现在要找一个eigenvector，能够maximize
$$
(w^1)^T S w^1
$$。



对上面的那个式子整理一下，变成（scalar乘矩阵可以提出来）：


$$
(w^1)^T S w^1=\alpha (w^1)^T w^1
$$

其中，
$$
(w^1)^T w^1 =1
$$
所以上式就等于\\(\alpha\\)。现在也就是要找到一个最大的\\(\alpha\\)。




**所以可以得出结论：\\(w^1\\)是covariance matrix S的eigenvector，同时对应到最大的eigenvalue \\(\lambda_1\\)**






## 找\\(w^2\\)




Optimization problem：找到一个\\(w^2\\)，能够最大化
$$
(w^2)^T S w^2
$$
，同时保证\\((w^2)^T w^2 =1\\)，且\\((w^2)^T w^1=0\\)。



**还是用拉格朗日数乘法：**



$$
g(w^2)=(w^2)^T S w^2-\alpha((w^2)^Tw^2-1)-\beta((w^2)^Tw^1-0)
$$

**求解偏微分：**


$$
\begin{cases}
\partial g(w^2)/\partial w^2_1=0 \\
\partial g(w^2)/\partial w^2_2=0 \\
\qquad \quad \vdots 
\end{cases}
$$

整理化简得到：

$$
S w^2-\alpha w^2 -\beta w^1=0
$$

左边同乘以\\((w^1)^T\\)，得到：

$$
(w^1)^T S w^2-\alpha(w^1)^T w^2 -\beta(w^1)^T w^1=0
$$
其中，显而易得：
$$
\begin{cases}
(w^1)^T w^2 =0 \\
(w^1)^T w^1 =0 
\end{cases}
$$

**至于第一项：**



默认来说这里的\\(w^1\\)和\\(w^2\\)都是列向量**？？？**



这么一来，
$$
(w^1)^T S w^2
$$
就是一行乘以一个矩阵再乘以一个一列，得到的是一个scalar，**对scalar做转置还是它自己。**所以，

$$
(w^1)^T S w^2=((w^1)^T S w^2)^T
$$

根据穿脱原则，上式就等于：
$$
(w^2)^T S^T w^1
$$
因为S这个矩阵是Symmetric的，它的转置还是它自己。所以可以得到：
$$
(w^2)^T S w^1
$$

前面讲了，
$$
S w^1=\lambda_1 w^1
$$

代入，将常数提出，得到：
$$
\lambda_1(w^2)^T w^1
$$


显然，这一项也等于0，所以上面的第一项也等于0。
$$
0-\alpha \cdot 0 -\beta \cdot 1=0
$$
**得到的结论是：\\(\beta\\)等于0.**



所以在下面这个式子中，\\(\beta\\)这一项就会被拿掉。

$$
S w^2 - \alpha w^2 -\beta w^1 =0
$$


得到：
$$
S w^2-\alpha w^2=0
$$

即：
$$
S w^2=\alpha w^2
$$



**可以看出，\\(w^2\\)也是矩阵S的一个eigenvector，对应到第二大的eigenvalue \\(\lambda_2\\)。（不能选eigenvalue最大的那个eigenvector，因为这样和\\(w_1\\)不是orthogonal的）**



属于同一特征值的特征向量有可能不正交，对称矩阵属于不同特征值的特征向量是正交的。









