---
tag: 机器学习
---





词向量的表达形式和模型有很多，除了最简单的1 of N编码，其他模型都希望能够在词向量中保留某种语法和语义上的信息。

## GloVe



GloVe使用词与词之间的共现信息（共同出现的次数）来表示两两单词之间的某种联系。假设\\(X\\)为共现词频矩阵，其中的元素\\(x_{ij}\\)表示单词j出现在单词i的环境（上下文，可以设置不同的窗口大小）中的次数。设词汇表大小为\\(N\\)，则这个共现矩阵的大小为N行N列。记：


$$
x_i = \sum_{j=1}^N x_{ij}
$$


即，\\(x_i\\)为第i行，也就是所有词出现在单词i环境中的次数的和。



另外，given单词i，出现单词j的概率如下，这个概率是已知的：


$$
P_{ij}=P(j|i) = \frac{x_{ij}}{x_i}
$$






GloVe的做法非常主观，灵活的变形推导出了词和词之间的关系，其作者考虑了3个单词i,j,k，考虑如下式子：


$$
f(v_i,v_j,v_k)= \frac{P_{ik}}{P_{jk}} \tag{1}
$$


把k和i,j同时考虑进来，即（1）式等号右边部分，可以发现如下规律：



![a1](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic2/04GloVe%E5%92%8CfastText/a1.png)



这个规律体现出了词与词之间的某种联系，现在如果能够找到一个f，使得单词i,j,k的词向量通过这个函数能够得到右边的部分，就能够通过学习来获得保留某种联系之后的词向量。要知道，等号右边的项是可以根据语料库去直接算出来的，而且这个f在设计好以后也是固定的已知的，变量（参数）只有词向量本身。现在就是要去设计一个函数f（不唯一），这就是GloVe的主观之处。



等号右边是除法，可以想到指数和对数变换。不妨首先设计一个减法（符合考虑两个向量相关性的要求）：


$$
f(v_i-v_j,v_k)= \frac{P_{ik}}{P_{jk}}
$$


把自变量变换成标量（右边是标量）：


$$
f((v_i-v_j)^Tv_k)= \frac{P_{ik}}{P_{jk}}
$$


让f是一个exp函数，把减法变成除法：


$$
f((v_i-v_j)^Tv_k)=f(v_i^Tv_k-v_j^Tv_k)=\exp(v_i^Tv_k-v_j^Tv_k)=\frac{\exp(v_i^Tv_k)}{\exp(v_j^Tv_k)} = \frac{P_{ik}}{P_{jk}} \tag{2}
$$




(2)式中分子分母形式相同，所以只取一个，也就是要让：


$$
exp(v_i^Tv_k)= P_{ik}
$$


让上式等号两边越接近越好，就能够学习出恰当的词向量。上式等号右边的是可以直接根据语料库算出来的。


$$
P_{ik} = \frac{x_{ik}}{x_i}=exp(v_i^Tv_k)
$$







等号两边同时取log:


$$
v_i^Tv_k=\log(x_{ik}) - \log(x_i)\tag{3}
$$





在（3）式中，等号左右都是标量，但是，等号左边具有对称性（互换j和k结果不变），而等号右边没有对称性，原因是有\\(x_i\\)，其中一个解决方法是添加偏置项，让（3）式变成：


$$
v_i^Tv_k = \log(x_{ik})-b_i-b_k 
$$


也就是：


$$
v_i^Tv_j +b_i +b_j = \log(x_{ij}) \tag{4}
$$


到这里为止，整个模型就从最开始的计算比值变成了(4)式，等号右边的项是可以通过语料库直接计算得到。等号左边是要训练的参数，要通过训练使得等号左边尽可能等于等号右边。这是一个regression model，使用平方差损失函数：


$$
\sum_{i,j=1}^V f(x_{ij})(v_i^Tv_j+b_i+b_j-log(x_{ij}))^2 \tag{5}
$$






其中，V表示词典大小，f是一个权重项，自变量是\\(x_{ij}\\)，意味着不同的共现词频的重要性是不一样的。f要满足以下几个条件：



* \\(f(0)=0\\)，这样的话如果两个词不共现损失函数就为0，减少计算量
* 非递减的，这样的话那些rare co-occurrences词就不会被overweighted。
* 当x过大时\\(f(x)\\)不太大，让frequent co-occurrences词不overweighted。



作者经过试验提供了如下函数（原文中\\(\alpha\\)为0.75）：


$$
f(x)=\begin{cases}
(x/x_{max})^{\alpha} & , x \lt x_{max} \\
1   & , otherwise
\end{cases}
$$




![a2](https://luc-website.oss-cn-hangzhou.aliyuncs.com/websitepic2/04GloVe%E5%92%8CfastText/a2.png)



另外，在原文中，同一个单词也是有两种表示的，在（5）式中的\\(v_j\\)看成是\\(\tilde{v_j}\\)，这里使用一种表示和两种表示的差距很小（由于初始化不同），如果看成是两种表示最后作者提供的方法是求和得到最终的词向量。



---



## fastText





fastText就是基于skip-gram模型（使用负采样或层序softmax），将每个中心词视为子词集合，并学习子词的词向量。



例如，对于一个单词"where"，子词集合就是"<wh"、"whe"、"her"、"ere"、"re>"和`"<where>"`，可以手动调整例如最小最大长度等参数。尖括号可以将作为前后缀的子词区分开来，例如"her"和`"<her>"`



使用负采样的skip-gram模型的损失函数如下：


$$
-\log P(w_o|w_c) = -\log\frac{1}{1+\exp (-u_o^Tv_c)}-\sum_{k=1,w_k\sim P(w)}^K \log\frac{1}{1+\exp(u^T_{i_k}v_c)} 
$$


在fasttext中，只要把上式中的\\(v_c\\)替换成c这个**中心词的所有子词向量的和**就行



fastText对于一些低频词和oov词汇（oov词汇也可以使用已有语料库中的子词向量的和表示）处理效果比较好。